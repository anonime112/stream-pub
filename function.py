import language_tool_python
import spacy
import textstat

nlp = spacy.load("fr_core_news_sm")
tool = language_tool_python.LanguageTool('fr')

mots_abstraits = [
    "gÃ©rer", "organiser", "coordonner", "dÃ©finir", "Ã©tablir", "optimiser",
    "concevoir", "planifier", "analyser", "mettre en Å“uvre", "implÃ©menter",
    "structurer", "configurer", "assurer", "piloter", "conduire", "dÃ©ployer",
    "amÃ©liorer", "Ã©valuer", "suivre"
]
verbes_techniques = [
    "compiler", "scripter", "virtualiser", "configurer", "dÃ©boguer", "crypter",
    "cloner", "monitorer", "tokeniser", "parser", "indexer", "authentifier"
]
# Liste de verbes abstraits (Ã  enrichir)
verbes_abstraits = set([
    "gÃ©rer", "assurer", "optimiser", "superviser", "soutenir", "amÃ©liorer",
    "maintenir", "contribuer", "favoriser"
])

# Liste de mots flous (Ã  enrichir aussi)
mots_flous = set([
    "correctement", "efficacement", "de maniÃ¨re appropriÃ©e", "optimal",
    "pertinent", "appropriÃ©", "bon fonctionnement"
])

verbes_facilitants = [
    "mettre en place", "vÃ©rifier", "sÃ©curiser", "formaliser", "documenter",
    "renforcer", "centraliser", "supprimer", "auditer", "remÃ©dier", "corriger",
    "justifier", "s'assurer", "Ã©tablir", "limiter", "instaurer", "rÃ©aliser",
    "clarifier", "rÃ©gulariser", "identifier", "rÃ©diger", "modifier",
    "procÃ©der Ã ", "activer", "concevoir", "diffuser", "revoir", "dÃ©ployer",
    "mettre Ã  jour", "implÃ©menter", "Ã©laborer", "contrÃ´ler", "Ã©tayer"
]

ressources_keywords = [
    "ordinateur", "logiciel", "application", "formulaire", "fichier",
    "document", "base de donnÃ©es", "matÃ©riel", "rÃ©seau", "support",
    "connexion", "email", "identifiant", "dossier", "clÃ© USB", "serveur"
]

subordonnants = [
    "si", "lorsque", "quand", "afin que", "bien que", "parce que", "puisque",
    "avant que", "aprÃ¨s que", "pendant que", "comme", "alors que", "tandis que"
]

texte = """
Nous recommandons aux :
1.	Risk Manager de sâ€™assurer que les provisions pour risques et charges intÃ©grÃ©es dans lâ€™EP22, sont liÃ©es Ã  des cas irrÃ©versibles de pertes opÃ©rationnelles;
2.	Mettre en place un contrÃ´le conjoint rÃ©alisÃ© par le Risk Manager (DRC) et la Direction FinanciÃ¨re et comptable (DFC) prÃ©alablement Ã  la transmission de lâ€™Ã©tat prudentiel EP22.
"""


def evaluer_faisabilite(tache):
    a = evaluer_complexite(tache)
    b = evaluer_difficulte_tache(tache)
    a = evaluer_complexite(tache)


def normaliser(val, max_val):
    return max(0, min(100, 100 - (val / max_val) * 100))


def score_comprehension(texte: str) -> dict:
    doc = nlp(texte)
    phrases = list(doc.sents)
    nb_phrases = len(phrases)
    nb_tokens = len([token for token in doc if token.is_alpha])

    longueur_moyenne = nb_tokens / nb_phrases if nb_phrases else 0

    nb_subordonnees = 0
    nb_passives = 0
    for token in doc:
        if "sub" in token.dep_:
            nb_subordonnees += 1
        if token.dep_ == "aux:pass":
            nb_passives += 1

    complexite = nb_subordonnees + nb_passives
    mots_difficiles = textstat.difficult_words(texte)
    score_lisibilite = max(0, min(100, textstat.flesch_reading_ease(texte)))

    # Normalisation (Ã  partir de seuils arbitraires raisonnables)
    score_longueur = normaliser(longueur_moyenne,
                                30)  # seuil max : 30 mots/phrase
    score_complexite = normaliser(complexite,
                                  10)  # seuil max : 10 sub/passives
    score_vocabulaire = normaliser(mots_difficiles,
                                   15)  # seuil max : 15 mots difficiles

    # Moyenne pondÃ©rÃ©e
    score_global = round((0.25 * score_longueur + 0.25 * score_complexite +
                          0.25 * score_vocabulaire + 0.25 * score_lisibilite),
                         2)

    return {
        "longueur_moyenne": round(longueur_moyenne, 2),
        "nb_subordonnees": nb_subordonnees,
        "nb_passives": nb_passives,
        "mots_difficiles": mots_difficiles,
        "score_lisibilite": score_lisibilite,
        "score_global_comprehension": score_global
    }


def evaluer_comprehension(texte):
    doc = nlp(texte)
    nb_phrases = len(list(doc.sents))
    nb_mots = len(doc)
    moyenne_longueur = nb_mots / nb_phrases if nb_phrases else 1
    lisibilite = textstat.flesch_reading_ease(texte)
    fautes = len(tool.check(texte))

    score = (
        0.4 *
        (1 - min(moyenne_longueur / 25, 1)) +  # phrases courtes = plus clair
        0.3 * min(lisibilite / 100, 1) +  # meilleure lisibilitÃ©
        0.3 * (1 - min(fautes / 5, 1))  # moins de fautes = plus comprÃ©hensible
    )
    return max(0.0, min(score, 1.0))


def evaluer_complexite(tache):
    doc = nlp(tache)
    nb_mots = len(doc)
    nb_verbes = sum(1 for token in doc if token.pos_ == "VERB")
    nb_subordonnees = sum(1 for token in doc
                          if token.dep_ in ["mark", "advcl", "relcl"])

    nb_abstraits = sum(1 for token in doc if token.lemma_ in mots_abstraits)

    lisibilite = textstat.flesch_reading_ease(tache)
    lisibilite_score = max(0, min(1, (lisibilite - 30) / 70))  # 0-1 inversÃ©

    # Score final normalisÃ© (0 = simple, 1 = complexe)
    score = (
        0.2 * (nb_mots / 20) +  # phrase longue
        0.2 * (nb_verbes / 3) + 0.2 * (nb_subordonnees / 2) + 0.2 *
        (nb_abstraits / 2) + 0.2 *
        (1 - lisibilite_score)  # plus difficile Ã  lire = + complexe
    )
    return score


def AnalyseTache(taches):
    for tache in taches:
        doc = nlp(tache)

        # --- ClartÃ© : test de lisibilitÃ© + fautes de grammaire ---
        lisibilite = textstat.flesch_reading_ease(tache)

        fautes = tool.check(tache)
        nb_fautes = len(fautes)
        clarte_score = max(0, min(1, (lisibilite - 30) /
                                  70))  # normalisÃ© entre 0 et 1

        # --- SpÃ©cificitÃ© : on vÃ©rifie le nombre de noms et adjectifs concrets ---
        noms_specifiques = [
            token for token in doc if token.pos_ in ["NOUN", "PROPN", "ADJ"]
        ]
        specificite_score = min(1.0, len(noms_specifiques) / 4)

        # --- Ressources citÃ©es : objets, personnes, lieux, outils... ---
        ressources = [
            ent.text for ent in doc.ents
            if ent.label_ in ["MISC", "ORG", "PERSON", "LOC", "PRODUCT"]
        ]
        ressources_score = 1.0 if ressources or any(t.pos_ == "NOUN"
                                                    for t in doc) else 0.0

        print(f"ðŸ§¾ TÃ¢che : {tache}")
        print(
            f"  - Score de clartÃ©       : {clarte_score:.2f} (Fautes: {nb_fautes})"
        )
        print(
            f"  - Score de spÃ©cificitÃ©  : {specificite_score:.2f} ({len(noms_specifiques)} Ã©lÃ©ments)"
        )
        print(
            f"  - Ressources dÃ©tectÃ©es  : {ressources if ressources else 'aucune'} â†’ Score: {ressources_score:.2f}"
        )
        print("")
        acteurs = set()
        actions = set()
        ressources = set()

        for chunk in doc.noun_chunks:
            print("Groupe nominal :", chunk.text)

            texte = chunk.text.strip()

            # Acteurs probables (par heuristique : contient "Manager", "Direction", etc.)
            if any(mot in texte.lower() for mot in
                   ["manager", "direction", "responsable", "service"]):
                acteurs.add(texte)
        # Ressources probables
            elif any(mot in texte.lower() for mot in [
                    "contrÃ´le", "Ã©tat", "transmission", "provision", "charge",
                    "perte"
            ]):
                ressources.add(texte)

        for token in doc:
            if token.dep_ in ("nsubj", "nmod", "pobj",
                              "appos") and token.pos_ in ("PROPN", "NOUN"):
                acteurs.add(token.text)
            if token.pos_ == "VERB":
                actions.add(token.lemma_)  # le verbe Ã  l'infinitif
            if token.dep_ in ("obj", "obl",
                              "nmod") and token.pos_ in ("NOUN", "PROPN"):
                ressources.add(token.text)

        print("ðŸ”¸ Phrase analysÃ©e :", tache)
        print("ðŸ‘¤ Personnes concernÃ©es :", ", ".join(acteurs)
              or "Aucune trouvÃ©e")
        print("âœ… Verbes dâ€™action :", ", ".join(actions) or "Aucun trouvÃ©")
        print("ðŸ“¦ Ressources associÃ©es :", ", ".join(ressources)
              or "Aucune trouvÃ©e")


def extraire_taches(texte: str) -> list[str]:
    """
    Extrait les tÃ¢ches d'un texte en dÃ©tectant les phrases qui contiennent
    des verbes Ã  l'infinitif ou Ã  l'impÃ©ratif.
    """
    doc = nlp(texte)

    taches = []

    for phrase in doc.sents:
        phrase_doc = nlp(phrase.text)
        for token in phrase_doc:
            if token.pos_ == "VERB" and token.morph.get("VerbForm") in [[
                    "Inf"
            ], ["Imp"]]:
                taches.append(phrase.text.strip())
                break  # une seule dÃ©tection de verbe suffit

    return taches


def evaluer_difficulte_tache(tache: str) -> dict:
    """
    Ã‰value la difficultÃ© d'une tÃ¢che en retournant un score (0 Ã  100)
    et les facteurs de difficultÃ© dÃ©tectÃ©s.
    """
    doc = nlp(tache)
    score = 0
    facteurs = []
    texte_min = tache.lower()

    # 1. Mots abstraits et verbes techniques = difficultÃ© â†‘
    for token in doc:
        lemma = token.lemma_.lower()
        if lemma in mots_abstraits:
            score += 15
            facteurs.append(f"mot abstrait : {token.text}")
        elif lemma in verbes_techniques:
            score += 20

    # 2. Verbes facilitants = difficultÃ© â†“
    for verbe in verbes_facilitants:
        if verbe in texte_min:
            score -= 10
    # 3. Ressources mentionnÃ©es ?
    if not any(r in texte_min for r in ressources_keywords):
        score += 10
    # 4. Subordonnants = complexitÃ© syntaxique â†‘
    if any(s in texte_min for s in subordonnants):
        score += 10

    # 5. Longueur de la phrase
    if len(doc) > 20:
        score += 10

    # 6. Nombre de mots difficiles (selon textstat)
    nb_difficiles = textstat.difficult_words(tache)
    score += nb_difficiles * 2  # pondÃ©ration ajustable
    if nb_difficiles > 0:
        facteurs.append(f"{nb_difficiles} mots difficiles")

    difficiles = textstat.difficult_words(tache)
    score = score + difficiles

    return score


taches = extraire_taches(texte)

for t in taches:
    s = evaluer_complexite(t)
    niveau = "Faible" if s < 0.3 else "Moyenne" if s < 0.7 else "Ã‰levÃ©e"
    print(f"ðŸ§¾ TÃ¢che : {t}")
    print(f"  - Score de complexitÃ© : {s:.2f} â†’ Niveau : {niveau}\n")
    resultat = evaluer_difficulte_tache(t)
    print(f"  -score_difficulte : {resultat:.2f} â†’ Niveau : {niveau}\n")

    # for m in resultat:
    #   print(f"{m}: {resultat[m]}")

    print(f"------------------------------------------------------------")

score = evaluer_comprehension(texte)
niveau = "Faible" if score < 0.3 else "Moyenne" if score < 0.7 else "Bonne"
print(f"------------------------------------------------------------")
print(f"ðŸ“ ComprÃ©hension du texte : Score = {score:.2f} â†’ Niveau : {niveau}")
comp = score_comprehension(texte)
for m in comp:
    print(f"{m}: {comp[m]}")
print(f"------------------------------------------------------------")
print(f"------analyse de la tache------")
# AnalyseTache(taches)

