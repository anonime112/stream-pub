import language_tool_python
import spacy
import textstat
import re

nlp = spacy.load("fr_core_news_sm")
tool = language_tool_python.LanguageTool('fr')


modalites_regex = [
    r"\bsi\b", r"\blorsque\b", r"\bapr√®s\b", r"\b√† condition que\b",
    r"\ben cas de\b", r"\bsous r√©serve\b", r"\bdu moment que\b",
    r"\baussit√¥t que\b", r"\bd√®s que\b"
]

verbes_etat = {
    "√™tre", "sembler", "para√Ætre", "devenir", "rester", "demeurer", "avoir l'air"
,
 "passer",
	"constituer",
	"repr√©senter",
"appartenir"
}

def compter_verbes_action_avances(text: str) -> int:
    doc = nlp(text)
    count = 0

    i = 0
    while i < len(doc):
        token = doc[i]

        # V√©rifie si c'est un verbe
        if token.pos_ == "VERB":
            # Cas particulier pour l'expression verbale "avoir l'air"
            if token.lemma_ == "avoir" and i + 1 < len(doc) and doc[i + 1].text.lower() == "l'air":
                i += 2  # On saute "avoir" + "l'air", consid√©r√© comme verbe d‚Äô√©tat
                continue

            lemme = token.lemma_.lower()

            # Si ce n‚Äôest pas un verbe d‚Äô√©tat
            if lemme not in verbes_etat:
                # V√©rifie la pr√©sence d'un sujet ou d'un objet
                has_subject = any(child.dep_ == "nsubj" for child in token.children)
                has_object = any(child.dep_ in ("obj", "dobj") for child in token.children)

                if has_subject or has_object:
                    count += 1

        i += 1

    return count


def detecter_modalites(text):
    """
    D√©tecte les modalit√©s (conditionnelles, temporelles, restrictives) dans un texte.
    Combine une approche par regex et une analyse syntaxique via spaCy.
    
    Retourne :
    - le nombre de modalit√©s d√©tect√©es
    - la liste des expressions trouv√©es
    """
    # Analyse avec spaCy
    doc = nlp(text)
    
    # 1. Par expressions r√©guli√®res
    texte_lower = text.lower()
    termes_trouves = []
    for pattern in modalites_regex:
        matches = re.findall(pattern, texte_lower)
        termes_trouves.extend(matches)
    
    # 2. Par analyse syntaxique (d√©tection des subordonnants conditionnels)
    for token in doc:
        if token.dep_ in ["mark", "advcl"] and token.text.lower() in {"si", "lorsque", "quand", "√† condition", "apr√®s"}:
            termes_trouves.append(token.text.lower())

    # Nettoyer les doublons
    termes_uniques = list(set(termes_trouves))
    
    return {
        "nbr": len(termes_uniques),
        "liste": termes_uniques,

    } 


mots_abstraits = [
    "g√©rer", "organiser", "coordonner", "d√©finir", "√©tablir", "optimiser",
    "concevoir", "planifier", "analyser", "mettre en ≈ìuvre", "impl√©menter",
    "structurer", "configurer", "assurer", "piloter", "conduire", "d√©ployer",
    "am√©liorer", "√©valuer", "suivre"
]
verbes_techniques = [
    "compiler", "scripter", "virtualiser", "configurer", "d√©boguer", "crypter",
    "cloner", "monitorer", "tokeniser", "parser", "indexer", "authentifier"
]
# Liste de verbes abstraits (√† enrichir)
verbes_abstraits = set([
    "g√©rer", "assurer", "optimiser", "superviser", "soutenir", "am√©liorer",
    "maintenir", "contribuer", "favoriser"
])

# Liste de mots flous (√† enrichir aussi)
mots_flous = set([
    "correctement", "efficacement", "de mani√®re appropri√©e", "optimal",
    "pertinent", "appropri√©", "bon fonctionnement"
])

verbes_facilitants = [
    "mettre en place", "v√©rifier", "s√©curiser", "formaliser", "documenter",
    "renforcer", "centraliser", "supprimer", "auditer", "rem√©dier", "corriger",
    "justifier", "s'assurer", "√©tablir", "limiter", "instaurer", "r√©aliser",
    "clarifier", "r√©gulariser", "identifier", "r√©diger", "modifier",
    "proc√©der √†", "activer", "concevoir", "diffuser", "revoir", "d√©ployer",
    "mettre √† jour", "impl√©menter", "√©laborer", "contr√¥ler", "√©tayer"
]

termes_flous = {
    "appropri√©", "ad√©quat", "suffisant", "rapide", "fiable", "efficace",
    "optimis√©", "correct", "important", "requis", "significatif", "am√©lior√©",
    "satisfaisant", "pertinent", "conforme", "en temps voulu", "appropri√©e",
    "adapt√©", "utile", "standard", "normal", "pr√©vu", "bon", "mauvais", "meilleur",  "correctement", "efficacement", "de mani√®re appropri√©e", "optimal",
    "pertinent", "appropri√©", "bon fonctionnement"
}

ressources_keywords = [
    "ordinateur", "logiciel", "application", "formulaire", "fichier",
    "document", "base de donn√©es", "mat√©riel", "r√©seau", "support",
    "connexion", "email", "identifiant", "dossier", "cl√© USB", "serveur"
]

subordonnants = [
    "si", "lorsque", "quand", "afin que", "bien que", "parce que", "puisque",
    "avant que", "apr√®s que", "pendant que", "comme", "alors que", "tandis que"
]

texte = """
ÔÇß Mettre en place un dispositif transversal d‚Äôidentification pr√©coce et de pr√©vention de la fraude. > DACP
ÔÇß Mettre en place un dispositif d‚Äôalerte risque de fraude int√©grant les sc√©narii de fraude et des contr√¥les applicatifs associ√©s (exemple : v√©rification des soldes et param√©trage de la s√©paration des t√¢ches).  > DACP
ÔÇß Elaborer un plan de formation/sensibilisation les sujets de fraudes dans la strat√©gie globale de gestion des risques.  > DACP
ÔÇß Faire appel √† un tiers sp√©cialis√© pour l‚Äôaudit de la configuration des crit√®res l'alerte mis en place par le prestataire.  > DACP
"""





def evaluer_comprehension(texte):
    doc = nlp(texte)
    nb_phrases = len(list(doc.sents))
    nb_mots = len(doc)
    moyenne_longueur = nb_mots / nb_phrases if nb_phrases else 1
    lisibilite = textstat.flesch_reading_ease(texte)
    fautes = len(tool.check(texte))

    score = (
        0.4 * (1 - min(moyenne_longueur / 25, 1)) +  # phrases courtes = plus clair
        0.3 * min(lisibilite / 100, 1) +             # meilleure lisibilit√©
        0.3 * (1 - min(fautes / 5, 1))               # moins de fautes = plus compr√©hensible
    )
    return max(0.0, min(score, 1.0))



def calculer_densite_ambiguite_lexicale(text: str) -> float:
    """
    Calcule la densit√© d‚Äôambigu√Øt√© lexicale (nb de termes flous / nb total de tokens).
    Retourne une valeur entre 0 et 1 (ou plus si le texte est tr√®s court et tr√®s flou).
    """
    doc = nlp(text)
    nb_tokens = len(doc)

    if nb_tokens == 0:
        return 0.0

    nb_flous = sum(1 for token in doc if token.lemma_.lower() in termes_flous)
    densite = nb_flous / nb_tokens

    return densite




def extraire_actions_ou_procedures(texte: str) -> list[str]:
    lignes = texte.splitlines()
    actions = []
    complexite=0
    difficulte=0

    # motif pour les sous-t√¢ches comme (i), (ii)
    motif_sous_taches = re.compile(r"\(\s?[ivxlcdm]+\s?\)\s*", re.IGNORECASE)
    # motif pour les bullets
    motif_puce = re.compile(r"^\s*[-‚Ä¢ÔÇß*‚Ä¢\d.]+\s*")
    motif_etape = re.compile(
    r"^(E?tape\s*\d+\s*:?|\bEape\d+\s*:?)",
    re.IGNORECASE
)

    for ligne in lignes:
        ligne_originale = ligne.strip()
        if not ligne_originale:
            continue

        # Gestion des sous-t√¢ches
        if motif_sous_taches.search(ligne_originale):
            sous_taches = re.split(motif_sous_taches, ligne_originale)
            intro = sous_taches[0].strip(": ")
            for sous in sous_taches[1:]:
                if sous.strip():
                    actions.append(f"{intro} : {sous.strip()}")
            continue

        # Suppression de la puce √©ventuelle
        ligne = motif_puce.sub("", ligne_originale)
        # Suppression de la puce √©ventuelle
        ligne = motif_etape.sub("", ligne_originale)


        doc = nlp(ligne)
        phrase = list(doc.sents)[0] if list(doc.sents) else nlp(ligne)

        # Cas 1 : Verbe d√©tect√© dans la phrase (devoir, infinitif, imp√©ratif, etc.)
        if any(
            token.lemma_ == "devoir"
            or "Inf" in token.morph.get("VerbForm")
            or "Imp" in token.morph.get("VerbForm")
            or token.pos_ == "VERB"
            for token in phrase
        ):
            actions.append(ligne)
            continue

        # Cas 2 : Phrase courte commen√ßant par un nom (action implicite)
        tokens = [token for token in phrase if not token.is_punct]
        if tokens and len(tokens) <= 7 and tokens[0].pos_ == "NOUN":
            actions.append(ligne)
            continue
        
    for t in actions:
        complexite = complexite+evaluer_complexite(t)
        difficulte = difficulte+evaluer_difficulte_tache(t)
 
        

    return {
        "longueur_moyenne": actions,
        "nb_phrase":len(actions) ,
         "complexite":complexite,
          "difficulte":difficulte ,
      
    }



def extraire_taches_regex_spacy(text: str) -> list[str]:
    lignes = text.splitlines()
    taches = []

    motif_tache_directe = re.compile(
        r"^(?:-|\*|ÔÇß|‚Ä¢|\d+\.)?\s*(?:Mettre en place|Sensibiliser|Faire appel|D√©finir|Revue|Accro√Ætre|√âlaborer|Veiller √†|Interfacer|Fixer|√âtablir|Formaliser|Inclure|Cr√©er|D√©ployer|Embarquer|Revoir|√âlargir|Assurer|Renforcer|Doit|Doivent|Devra|Traiter|Proc√©der|D√©terminer)",
        re.IGNORECASE
    )

    motif_sous_taches = re.compile(r"\(\s?[ivxlcdm]+\s?\)\s", re.IGNORECASE)  # (i) (ii) (iii)...

    for ligne in lignes:
        ligne = ligne.strip()
        if not ligne:
            continue

        # Cas 1 : Ligne avec plusieurs sous-t√¢ches entre (i)...(ii)...(iii)...
        if motif_sous_taches.search(ligne):
            sous_taches = re.split(r"\(\s?[ivxlcdm]+\s?\)\s*", ligne)
            phrase_intro = sous_taches[0].strip(": \n")
            for sous in sous_taches[1:]:
                if sous.strip():
                    taches.append(f"{phrase_intro} : {sous.strip()}")
            continue

        # Cas 2 : ligne directe qui commence par un verbe d'action
        if motif_tache_directe.search(ligne):
            taches.append(ligne)
            continue

        # Cas 3 : analyse grammaticale
        doc = nlp(ligne)
        for phrase in doc.sents:
            texte_phrase = phrase.text.strip()
            doc_phrase = nlp(texte_phrase)
            for token in doc_phrase:
                formes = token.morph.get("VerbForm")
                if token.lemma_ == "devoir" or "Inf" in formes or "Imp" in formes:
                    taches.append(texte_phrase)
                    break

    return {
        "longueur_moyenne": taches,
        "nb_phrase":len(taches) ,
      
    }


def normaliser(val, max_val):
    return max(0, min(100, 100 - (val / max_val) * 100))


def score_comprehension(texte: str) -> dict:
    doc = nlp(texte)
    phrases = list(doc.sents)
    nb_phrases = len(phrases)
    nb_tokens = len([token for token in doc if token.is_alpha])

    longueur_moyenne = nb_tokens / nb_phrases if nb_phrases else 0

    nb_subordonnees = 0
    nb_passives = 0
    for token in doc:
        if "sub" in token.dep_:
            nb_subordonnees += 1
        if token.dep_ == "aux:pass":
            nb_passives += 1

    complexite = nb_subordonnees + nb_passives
    mots_difficiles = textstat.difficult_words(texte)
    score_lisibilite = max(0, min(100, textstat.flesch_reading_ease(texte)))

    # Normalisation (√† partir de seuils arbitraires raisonnables)
    score_longueur = normaliser(longueur_moyenne,
                                30)  # seuil max : 30 mots/phrase
    score_complexite = normaliser(complexite,
                                  10)  # seuil max : 10 sub/passives
    score_vocabulaire = normaliser(mots_difficiles,
                                   15)  # seuil max : 15 mots difficiles

    # Moyenne pond√©r√©e
    score_global = round((0.25 * score_longueur + 0.25 * score_complexite +
                          0.25 * score_vocabulaire + 0.25 * score_lisibilite),
                         2)

    return {
        "longueur_moyenne": round(longueur_moyenne, 2),
        "nb_subordonnees": nb_subordonnees,
        "nb_passives": nb_passives,
        "mots_difficiles": mots_difficiles,
        "score_lisibilite": score_lisibilite,
        "score_global_comprehension": score_global
    }


def evaluer_complexite(tache):
    doc = nlp(tache)
    nb_mots = len(doc)
    nb_verbes = sum(1 for token in doc if token.pos_ == "VERB")
    nb_subordonnees = sum(1 for token in doc
                          if token.dep_ in ["mark", "advcl", "relcl"])

    nb_abstraits = sum(1 for token in doc if token.lemma_ in mots_abstraits)

    lisibilite = textstat.flesch_reading_ease(tache)
    lisibilite_score = max(0, min(1, (lisibilite - 30) / 70))  # 0-1 invers√©

    # Score final normalis√© (0 = simple, 1 = complexe)
    score = (
        0.2 * (nb_mots / 20) +  # phrase longue
        0.2 * (nb_verbes / 3) + 0.2 * (nb_subordonnees / 2) + 0.2 *
        (nb_abstraits / 2) + 0.2 *
        (1 - lisibilite_score)  # plus difficile √† lire = + complexe
    )
    return score


def AnalyseTache(taches):
    for tache in taches:
        doc = nlp(tache)

        # --- Clart√© : test de lisibilit√© + fautes de grammaire ---
        lisibilite = textstat.flesch_reading_ease(tache)

        fautes = tool.check(tache)
        nb_fautes = len(fautes)
        clarte_score = max(0, min(1, (lisibilite - 30) /
                                  70))  # normalis√© entre 0 et 1

        # --- Sp√©cificit√© : on v√©rifie le nombre de noms et adjectifs concrets ---
        noms_specifiques = [
            token for token in doc if token.pos_ in ["NOUN", "PROPN", "ADJ"]
        ]
        specificite_score = min(1.0, len(noms_specifiques) / 4)

        # --- Ressources cit√©es : objets, personnes, lieux, outils... ---
        ressources = [
            ent.text for ent in doc.ents
            if ent.label_ in ["MISC", "ORG", "PERSON", "LOC", "PRODUCT"]
        ]
        ressources_score = 1.0 if ressources or any(t.pos_ == "NOUN"
                                                    for t in doc) else 0.0

        print(f"üßæ T√¢che : {tache}")
        print(
            f"  - Score de clart√©       : {clarte_score:.2f} (Fautes: {nb_fautes})"
        )
        print(
            f"  - Score de sp√©cificit√©  : {specificite_score:.2f} ({len(noms_specifiques)} √©l√©ments)"
        )
        print(
            f"  - Ressources d√©tect√©es  : {ressources if ressources else 'aucune'} ‚Üí Score: {ressources_score:.2f}"
        )
        print("")
        acteurs = set()
        actions = set()
        ressources = set()

        for chunk in doc.noun_chunks:
            print("Groupe nominal :", chunk.text)

            texte = chunk.text.strip()

            # Acteurs probables (par heuristique : contient "Manager", "Direction", etc.)
            if any(mot in texte.lower() for mot in
                   ["manager", "direction", "responsable", "service"]):
                acteurs.add(texte)
        # Ressources probables
            elif any(mot in texte.lower() for mot in [
                    "contr√¥le", "√©tat", "transmission", "provision", "charge",
                    "perte"
            ]):
                ressources.add(texte)

        for token in doc:
            if token.dep_ in ("nsubj", "nmod", "pobj",
                              "appos") and token.pos_ in ("PROPN", "NOUN"):
                acteurs.add(token.text)
            if token.pos_ == "VERB":
                actions.add(token.lemma_)  # le verbe √† l'infinitif
            if token.dep_ in ("obj", "obl",
                              "nmod") and token.pos_ in ("NOUN", "PROPN"):
                ressources.add(token.text)

        print("üî∏ Phrase analys√©e :", tache)
        print("üë§ Personnes concern√©es :", ", ".join(acteurs)
              or "Aucune trouv√©e")
        print("‚úÖ Verbes d‚Äôaction :", ", ".join(actions) or "Aucun trouv√©")
        print("üì¶ Ressources associ√©es :", ", ".join(ressources)
              or "Aucune trouv√©e")




def evaluer_difficulte_tache(tache: str) -> dict:
    """
    √âvalue la difficult√© d'une t√¢che en retournant un score (0 √† 100)
    et les facteurs de difficult√© d√©tect√©s.
    """
    doc = nlp(tache)
    score = 0
    facteurs = []
    texte_min = tache.lower()

    # 1. Mots abstraits et verbes techniques = difficult√© ‚Üë
    for token in doc:
        lemma = token.lemma_.lower()
        if lemma in mots_abstraits:
            score += 15
            facteurs.append(f"mot abstrait : {token.text}")
        elif lemma in verbes_techniques:
            score += 20

    # 2. Verbes facilitants = difficult√© ‚Üì
    for verbe in verbes_facilitants:
        if verbe in texte_min:
            score -= 10
    # 3. Ressources mentionn√©es ?
    if not any(r in texte_min for r in ressources_keywords):
        score += 10
    # 4. Subordonnants = complexit√© syntaxique ‚Üë
    if any(s in texte_min for s in subordonnants):
        score += 10

    # 5. Longueur de la phrase
    if len(doc) > 20:
        score += 10

    # 6. Nombre de mots difficiles (selon textstat)
    nb_difficiles = textstat.difficult_words(tache)
    score += nb_difficiles * 2  # pond√©ration ajustable
    if nb_difficiles > 0:
        facteurs.append(f"{nb_difficiles} mots difficiles")

    difficiles = textstat.difficult_words(tache)
    score = score + difficiles

    return score



taches1 = extraire_taches_regex_spacy(texte)

for t in taches1:
    s = evaluer_complexite(t)
    niveau = "Faible" if s < 0.3 else "Moyenne" if s < 0.7 else "√âlev√©e"
    print(f"üßæ T√¢che : {t}")
    print(f"  - Score de complexit√© : {s:.2f} ‚Üí Niveau : {niveau}\n")
    resultat = evaluer_difficulte_tache(t)
    print(f"  -score_difficulte : {resultat:.2f} ‚Üí Niveau : {niveau}\n")

    # for m in resultat:
    #   print(f"{m}: {resultat[m]}")

    print(f"------------------------------------------------------------")

score = evaluer_comprehension(texte)
niveau = "Faible" if score < 0.3 else "Moyenne" if score < 0.7 else "Bonne"
print(f"------------------------------------------------------------")
print(f"üìù Compr√©hension du texte : Score = {score:.2f} ‚Üí Niveau : {niveau}")
comp = score_comprehension(texte)
for m in comp:
    print(f"{m}: {comp[m]}")
print(f"------------------------------------------------------------")
print(f"------analyse de la tache------")
print(len(taches1))
print(taches1["nb_phrase"])
# AnalyseTache(taches)


